model:
    family: transformer
    n_embd: 256
    n_layer: 12
    n_head: 8
    n_dims: 20
    n_positions: 101

train:
    data: gaussian
    task_kwargs: {}
    batch_size: 64
    learning_rate: 0.0001
    save_every_steps: 1000
    keep_every_steps: 10000
    train_steps: 50001
    curriculum:
        dims:
            start: 5
            end: 20
            inc: 1
            interval: 2000